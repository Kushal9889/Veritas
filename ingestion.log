2026-02-21 13:45:42 | INFO | scripts.ingest_all | Starting bulk ingestion {"root": "data/raw"}
2026-02-21 13:45:48 | INFO | chromadb.telemetry.product.posthog | Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-21 13:45:51 | INFO | core.models | Batch embedding succeeded {"count": 100}
2026-02-21 13:45:54 | INFO | core.models | Batch embedding succeeded {"count": 100}
2026-02-21 13:45:56 | INFO | core.models | Batch embedding succeeded {"count": 100}
2026-02-21 13:45:58 | INFO | core.models | Batch embedding succeeded {"count": 100}
2026-02-21 13:45:59 | INFO | core.models | Batch embedding succeeded {"count": 100}
2026-02-21 13:46:00 | INFO | core.models | Batch embedding succeeded {"count": 79}
2026-02-21 13:46:00 | INFO | ingestion.storage | Ingestion completed {"successful_batches": 6, "total_batches": 6, "duration_s": 12.65}
2026-02-21 13:46:02 | INFO | neo4j.notifications | Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` has no effect.} {description: `CONSTRAINT constraint_1720359a FOR (e:Entity) REQUIRE (e.id) IS UNIQUE` already exists.} for query: 'CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE'
2026-02-21 13:46:02 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:04 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:04 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:05 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:06 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:07 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:08 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:09 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:10 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:10 | ERROR | ingestion.storage | Graph DB node mutation failure {"error": "{code: Neo.ClientError.Schema.ConstraintValidationFailed} {message: Node(92) already exists with label `Entity` and property `id` = 'Annual Report on Form 10-K'}"}
2026-02-21 13:46:11 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:11 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:13 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:15 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:17 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:19 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:20 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-21 13:46:21 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:46:21 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99905, Requested 493. Please try again in 5m43.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:46:34 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:46:34 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:46:34 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:46:34 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99889, Requested 516. Please try again in 5m49.919999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:46:41 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:46:41 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:46:41 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:46:41 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99881, Requested 511. Please try again in 5m38.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:46:48 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:46:48 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:46:48 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:46:48 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99873, Requested 479. Please try again in 5m4.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:46:58 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:46:58 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:46:58 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:46:58 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99862, Requested 520. Please try again in 5m30.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:03 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:03 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:03 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:03 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99856, Requested 531. Please try again in 5m34.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:09 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:09 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:09 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:09 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99849, Requested 502. Please try again in 5m3.264s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:19 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:19 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:19 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:19 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99837, Requested 500. Please try again in 4m51.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:27 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:27 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:27 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:27 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99828, Requested 406. Please try again in 3m22.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:37 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:37 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:37 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:37 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99816, Requested 519. Please try again in 4m49.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:44 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:44 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:44 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:44 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99808, Requested 533. Please try again in 4m54.623999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:47:50 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:47:50 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:47:50 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:47:50 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99801, Requested 528. Please try again in 4m44.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
2026-02-21 13:48:00 | WARNING | ingestion.storage | Graph extraction failed via Gemini, falling back to Ollama {"error": "'NoneType' object has no attribute 'invoke'"}
2026-02-21 13:48:00 | ERROR | ingestion.storage | Ollama fallback failed {"error": "extract_graph_from_text.<locals>._ollama_extract() missing 1 required positional argument: 'model_name'"}
2026-02-21 13:48:00 | INFO | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-02-21 13:48:00 | WARNING | ingestion.storage | Groq extraction failed {"error": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01khs484t5ft8tj4xkthync4st` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99789, Requested 516. Please try again in 4m23.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"}
